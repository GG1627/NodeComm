# SynapseNet â€“ Interactive 3D Hardware Atlas & Chaos Survival Challenge

## ğŸš€ Overview

SynapseNet is an **interactive, AI-powered 3D dashboard** that reimagines what it means to be connected.  
We simulate hardware systems (CPUs, GPUs, memory, switches, interconnects like PCIe/NVLink/CXL) as a **living digital nervous system**.

- In **Learning Mode**, the system is an **explorable 3D atlas**:

  - Users can grab and move nodes (via hand gestures or mouse).
  - Removing or rerouting nodes shows real-time changes in latency, throughput, and resilience.
  - Acts as a **teaching tool** for people learning how hardware components communicate.

- In **Chaos Mode**, the system becomes a **survival challenge**:
  - An adversarial AI injects failures, overloads, and anomalies.
  - Users must use hand gestures (Cut, Heal, Reroute, Shield) to keep the system alive for 60s.
  - The system fights back dynamically; resilience score determines win/lose.

This project is **fun, visually stunning, and useful**. Itâ€™s both a learning platform and a performance piece.

---

## ğŸ¯ Core Goals

1. **Reimagine connection**: Make hardware interactions feel alive and explorable.
2. **Make AI the star**: AI injects chaos, detects anomalies, predicts failures, and reroutes intelligently.
3. **Enable interactivity**: Users control the system via gestures (computer vision) or fallback keyboard.
4. **Provide quantifiable results**: Metrics show resilience, throughput, downtime, and learning feedback.
5. **Deliver wow factor**: Judges see glowing 3D visuals, live chaos battles, and immersive sound.

---

## ğŸ§© Features

### 1. Telemetry Simulation

- Nodes: CPUs, GPUs, memory banks, switches.
- Links: PCIe, NVLink, CXL, Fabric.
- Metrics per link: latency, bandwidth, errors, utilization, status.
- Procedural simulation (AR(1) + Poisson spikes).
- Chaos injection: random degradation bursts.

### 2. Learning Mode (Atlas)

- 3D cluster rendered as glowing galaxy.
- Hover over a node â†’ tooltip explaining its role.
- Remove a node â†’ bandwidth redistributes, latency spikes.
- Move nodes â†’ visual + metric changes update in real time.
- Side panel explains â€œwhat just happenedâ€ in plain English.

### 3. Chaos Mode (Survival Challenge)

- AI adversary injects anomalies:
  - Latency spikes
  - Link overloads
  - Node isolation
- Player uses gestures:
  - âœŠ Cut â†’ isolate bad link
  - âœ‹ Heal â†’ repair link
  - ğŸ‘‰ Reroute â†’ force traffic around issue
  - ğŸ™Œ Shield â†’ global resilience boost
- Difficulty:
  - Easy = turn-based, 5s to respond
  - Medium = hybrid, shorter windows
  - Hard = real-time continuous attacks
- Win = survive 60s with resilience > threshold.

### 4. Computer Vision Controls

- Powered by **MediaPipe Hands** or **TensorFlow.js handpose**.
- Gestures detected:
  - âœŠ closed fist
  - âœ‹ open palm
  - ğŸ‘‰ index finger extended
  - ğŸ™Œ both hands open
- Mapped to backend control routes.
- Keyboard fallback: C, H, R, S.

### 5. 3D Visualization

- **React + Three.js + react-three-fiber**.
- Nodes = spheres with glowing emissive shaders.
- Links = curved tubes with flowing particle trails.
- Particles = data packets moving along connections.
- Animations:
  - Red edges = flicker + sparks
  - Heal = glowing wave
  - Reroute = wormhole effect
  - Shield = radial bloom
- Background = cosmic gradient (dark â†’ blue/teal glows).

### 6. Metrics & Feedback

- Resilience Gauge (0â€“100).
- Downtime %, Recovery time, Throughput gain %.
- Event log (AI attack, player action, resolution).
- Floating score deltas (+8, -10) near gauge.
- Post-game summary screen.

### 7. Audio Layer

- **Tone.js** for sound design.
- Healthy system = ambient harmony.
- Attack = glitch/distortion.
- Heal = chord resolution.
- Reroute = arpeggio sweep.
- Win/Lose = triumphant vs. collapse sound.

---

## ğŸ› ï¸ Tech Stack

**Backend**

- Python 3.11
- FastAPI + WebSockets
- Uvicorn
- Numpy, Pandas
- Scikit-learn (IsolationForest for anomalies)
- pyserial (for optional Arduino/DE10 telemetry)
- Prophet/PyTorch (stretch: predictive reroute)
- Dockerized

**Frontend**

- Next.js (TypeScript, App Router)
- Tailwind CSS
- Three.js + react-three-fiber + drei
- Zustand (state management)
- Cytoscape.js (for simple 2D graph view)
- Recharts (metrics panel)
- Framer Motion (animations)
- Tone.js (audio)

**Computer Vision**

- TensorFlow.js (browser handpose model)
- OR MediaPipe Hands (Python bridge â†’ WS events)

**Deployment**

- Docker Compose: backend + frontend
- Configurable via `.env`

---

## ğŸ“‚ Repo Structure

synapsenet/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ main.py # FastAPI entry
â”‚ â”‚ â”œâ”€â”€ schemas.py # TelemetryFrame, ActionLog, Scorecard
â”‚ â”‚ â”œâ”€â”€ providers/ # Data sources
â”‚ â”‚ â”‚ â”œâ”€â”€ sim.py
â”‚ â”‚ â”‚ â”œâ”€â”€ arduino.py
â”‚ â”‚ â”‚ â””â”€â”€ de10lite.py
â”‚ â”‚ â”œâ”€â”€ anomaly/ # Detection algorithms
â”‚ â”‚ â”‚ â”œâ”€â”€ rolling.py
â”‚ â”‚ â”‚ â””â”€â”€ isolation_forest.py
â”‚ â”‚ â”œâ”€â”€ optimize/ # Rerouting
â”‚ â”‚ â”‚ â”œâ”€â”€ rules.py
â”‚ â”‚ â”‚ â””â”€â”€ qlearn.py
â”‚ â”‚ â”œâ”€â”€ kpi/scorecard.py # Metrics
â”‚ â”‚ â”œâ”€â”€ replay/ # Pre-canned demo traces
â”‚ â”‚ â””â”€â”€ util/
â”‚ â””â”€â”€ tests/
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ app/dashboard/ # Main UI
â”‚ â”‚ â”œâ”€â”€ components/ # NervousGraph, GalaxyView, MetricsPanel, etc.
â”‚ â”‚ â”œâ”€â”€ lib/ # ws.ts, schema.ts
â”‚ â”‚ â””â”€â”€ styles/ # Tailwind config
â”‚
â”œâ”€â”€ research/ # Notes, models, experiments
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â””â”€â”€ .env.example

---

## âš™ï¸ Setup

### Backend

```bash
cd backend
uv init --name synapsenet-backend
uv add fastapi "uvicorn[standard]" pydantic
uv add numpy pandas scikit-learn
uv add pyserial python-dotenv orjson
uv run uvicorn src.main:app --reload --port 8000
```

cd frontend
npx create-next-app@latest . --typescript --eslint --tailwind --src-dir --app --import-alias "@/\*"
npm i three @react-three/fiber @react-three/drei zustand cytoscape react-cytoscapejs recharts framer-motion tone
npm run dev
